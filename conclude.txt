







Photo-realistic image synthesis using GAN

2017.9~2018.1

 
目录
1.	CycleGAN	2
1.1.	模型	2
1.2.	任务	2
1.3.	探索	3
1.3.1.	平衡生成器和判别器的能力	3
1.3.2.	多样性	3
1.3.3.	Checkerboard效应	5
1.3.4.	寻找重构误差的替代	7
2.	FaderNets	9
2.1.	模型	9
2.2.	任务	10
2.3.	探索	10
2.3.1.	FaderNets	10
2.3.2.	FaderGAN	14
3.	PGGAN	16
3.1.	模型	16
3.2.	任务	18
3.3.	探索	18
4.	Pix2pixHD	23
4.1.	模型	23
4.2.	任务	23
4.3.	探索	24


 
	CycleGAN
	模型
 
CycleGAN模型比较简单，上图是一个简单的示例。文章出自：
https://arxiv.org/pdf/1703.10593.pdf
相关代码在91服务器上，总共4个目录：
/data/rui.wu/gapeng/CycleGAN（早期版本，代码比较乱）
/data/rui.wu/gapeng/cyclegan_new/（重构版本，实验参数记录在档[exp/*/Models/opt.txt]）
/data/rui.wu/gapeng/cyclecgan/（改进的CycleGAN，可以做属性强弱插值）
/data/rui.wu/gapeng/cyclecgan_perceptual/（探索重构误差的替代loss，未做详尽探索）
	任务
围绕CycleGAN，我主要做两个任务：
其一，小黄人上色，这个数据集是配对数据，而且比较简单，小黄人的特点鲜明。这个任务主要用于测试模型。
下图每一行从左到右分别是原图(A/B)，翻译图(A->B/B->A)，翻译图翻回原图(A->B->A/B->A->B)。
 
 
其二，季节转换，这个数据集是自己收集的四季图片，其中对夏季和秋季做了标注。我主要研究这个数据集。

	探索
	平衡生成器和判别器的能力
GAN的训练比较难，原因在于生成器和判别器的能力容易失衡，导致一方太强。如果判别器能力太强，会导致判别器能够直接判断真假，因此无法给生成器提供足够的指导（注意到生成器的目标是让判别器把假样本判为真，但是判别器太强，这个目标没办法实现）。这就会引起梯度消失。而如果生成器太强，则能够针对判别器的弱点进行“攻击”，则容易一直针对某种范式进行生成，而这个范式可能在人看来是很奇怪的（比如人脸生成实验可能会产生鬼脸），容易产生mode collapse。
平衡G和D的能力方法有很多，包括换目标函数（换成其他GAN），引入noise，引入新的网络结构等。
(1). 换目标函数，原始的GAN容易产生很大的震荡，一个原因在于目标函数是BCE，交叉熵的梯度强度是比较大的。实验发现LSGAN在季节转换任务上更稳定一些。
(2). 引入noise，判别器判断真假的时候，给真样本引入noise，这相当于人为地制造不是很真的真样本，能够一定程度上限制判别器的能力。如果实验没有崩，建议不要引入noise作为干扰。
(3). 引入新的网络结构，这里有两个做法，其一，判别器换成PatchGAN，这不是每个任务都适用，只有数据在局部和全局上都能判断真假的情况下才可以用。多个patch判断真假，能够给G提供更多的指导；其二，换用精细的生成器和判别器。目前精细的生成器可以采用PGGAN的形式（pix2pixHD的coarse2fine其实也是一种progressive growing的方式）。精细的判别器可以采用multi-scale的判别器。样本经过下采样提供给负责低分辨率任务的判别器。后者在CycleGAN中没有尝试过，季节转换比较简单，不需要精细的生成器和判别器就能做到不错的效果。

	多样性
多样性有两种，一种是多模态，一种是单模态下属性强弱的变化。
CycleGAN是一个特定的映射，不具有多样性，就连属性强弱也不能修改。强行给生成器引入noise并不能获得多样性，这跟cycle和重构有关。
对于多模态多样性，现有的方法有2种，都来自于BicycleGAN（Toward Multimodal Image-to-Image Translation），其一是利用VAE去引入noise，这个方法称为cVAE-GAN；其二是引入noise，然后在翻译的图像上把noise预测出来，这个方法称为cLR-GAN，它容易崩。第二个方法尝试过，但是没起作用。
对于属性强弱多样性，现有CycleGAN框架是做不到的，可以做以下修改来实现。我把这个改进的CycleGAN方法称为CycleCGAN。CycleCGAN把域当作condition，仅需要一个G和一个D即可。
y ̅=G(x,1)
x^'=G(y ̅,-1)
x ̃=G(x,-1)
x ̅=G(y,-1)
y^'=G(x ̅,1)
y ̃=G(y,-1)
其中，相比CycleGAN而言，引入了额外的两个自监督项（第三个和第六个），这两个项的作用在于确保如果输入的图像已经是目标域的图像，则图像不用修改（至少应该保证不能被转换成其他域的结果），因此，引入的两项的loss为
‖x-x ̃ ‖+‖y-y ̃ ‖

实验效果（LSGAN，+exponential noise，Generator：autoencoder with skip connection，详细配置参见91服务器/data/rui.wu/gapeng/cyclecgan/exp/2017-11-09 112943/Models/opt.txt），每一行图片从左到右：原图、转为另一个季节、转回原季节、自监督(no op)。更多更好的实验结果，请参看/data/rui.wu/gapeng/cyclecgan/exp/目录。测试集上的结果请看每个实验下的Test目录。
秋->夏
 
 
 
夏->秋
 
 
 

	Checkerboard效应
CycleGAN在季节任务上市比较容易产生效果的，它学到的主要能力就是比较协调地“换颜色”。但是实验容易产生checkerboard效应。如下图所示。
 
针对checkerboard，主要探索了两种解决方案。
(1). 第一种方法是调参。CycleGAN的作者Junyanzhu也说过，迭代次数足够多，判别器能够发现checkerboard，并把它作为判别真假的依据，这样就能引导生成器去避开生成checkerboard。通过调参，并且引入了逐渐减弱的noise以后，可以得到不错的结果，但是仍然存在一点checkerboard。此外，调参还包括换网络结构，采用PatchGAN。
Iter=500000，LSGAN + exponential noise + G: autoencoder with skip connection(U-net)，具体实验配置参见/data/rui.wu/gapeng/cyclegan_new/exp/2017-11-07 162254/Models/opt.txt
每一行图片从左到右：原图、转为另一个季节、转回原季节。
秋->夏
 
 
 
夏->秋
 
 
 

(2). 第二种方案是寻找deconv的替代品。据研究（https://distill.pub/2016/deconv-checkerboard/），checkerboard现象主要来自于反卷积操作（convolution transpose），该操作非常容易出现overlap，并且是非常有规律的overlap，其规律类似于棋盘。
 
因此，要解决checkerboard，需要寻找反卷积的替代品。我们使用反卷积，主要用途是增大输入的尺寸，或者称为上采样。上采样的方法有很多，nearest upsampling，bilinear upsampling，cubic upsampling等等。现有框架支持的操作有nearest和bilinear。此外，还找到一个用channel去填充图像的长宽的做法，叫pixel shuffle。Pixel shuffle将大小为(N,C*k^2,H,W)的filter resize成(N,C,kH,kW)的filter,不引入额外的参数。
实验发现，采用nearest upsample + conv能够有效去除checkerboard，bilinear+conv也可以，但是pixel shuffle则失败了。
采用nearest upsample+conv替代deconv的实验结果如下：
 
 

此外，在季节转换任务上，我们发现虽然没有了checkerboard，但是却可能会产生“洞”。这个目前还没有什么优雅的解决方法，将反卷积和nearest upsample+conv的结果加权替换反卷积层可能能够移除这些“洞”。这个实验还没有做。
 

	寻找重构误差的替代
我想到了两种可能可行的替代方案。之所以要替代重构误差，是因为它限制了CycleGAN的形变能力。然而，去掉重构误差有可能带来id丢失的情况。
重构误差的一个可能的替代品是perceptual loss，它是一定程度上保持图像的内容。实验结果如下（91服务器/data/rui.wu/gapeng/cyclecgan_perceptual/exp/2017-11-12 172418/）：
 
可以看到，图像很模糊，这是因为perceptual loss比重构误差要弱好多。进一步的调参有可能得到更好的结果，但是模糊应该是不可避免的，毕竟perceptual loss相比重构误差要少了很多信息。

另一个可能可行的方案是将图像内容和域属性分离开来（一部分channel编码内容，一部分编码域属性），这样通过交换域属性就能得到翻译结果。但是我们也能够想到，如果图像不是配对的，这样交换会产生问题，也就是说它不分离内容和域属性，而是直接把整张图编码到域属性的channel里面，这样交换相当于直接交换两张图——这并不是我们想要的。即使采用GeneGAN，它仍然可能出现问题，域属性是编码在一些channel中，这些信息可能是跟内容相关的，我们能想到的就是，它把图像每个区域的季节风格编码到域属性channel中了，但是图像不是配对的，交换可能产生内容和风格不匹配的问题，导致一些区域出现很奇怪的季节风格，这给优化带来了很大的困难。
 
实际实验发现正是如此。下图是一个结果，更多实验参看91服务器/data/rui.wu/gapeng/IcGAN_CycleGAN/。
 

另外，我们小黄人数据来说，它是配对数据，能够得到还不错的效果。但是这种交换的实验，pair数据其实是很作弊的，在测试阶段几乎没办法做：
 

	FaderNets
	模型
FaderNets模型（论文原版模型）：
在特征层面对抗，剥离属性和内容。
 
FaderGAN模型（修改FaderNets）：
在图像层面对抗，在隐层添加属性。
 

论文：
Fader Networks:Manipulating Images by Sliding Attributes
代码：
官方代码：https://github.com/facebookresearch/FaderNetworks
复现代码：
91服务器：/data/rui.wu/gapeng/fader_nets/
125服务器：/data/bo718.wang/gapeng/fadergan/

	任务
(1). 复现FaderNets的效果，分辨率256x256。在StarGAN出来以前，它是state-of-the-art，能够将图像分辨率做到256，并且还能保持不错的效果。
(2). 从FaderNets衍生发展出FaderGAN，尝试做出好的效果。

	探索
	FaderNets
对于FaderNets，经过非常多的探索，发现文中采用2个channel编码属性的有和没有，也就是采用dummy的方式，比不采用dummy的方式要好。Dummy的方式是指编码为01和10，而非dummy的方式是指编码为0和1。不采用dummy的编码，实验一直没有效果，而采用dummy之后，尽管没能得到跟文章一样的效果，但是属性修改起作用了。
下图是FaderNets的实验结果（依次是原图、重构图、属性取反图），性别转换，可以看到能够换性别，但是图像比较模糊。
 
 

下图是FaderNets加上图像层面的对抗loss的实验结果图像仍然很模糊。
 
 

下图是引入perceptual loss的结果（属性：性别+眼镜）：
 
 
可以看到眼镜也有一点点效果。但是perceptual loss带来了其他类型的模糊，跟前面的重构误差带来的高斯模糊不一样。

属性修改的效果不明显，可以通过增大label的强度来使得效果更明显：
下图是加眼镜实验，每一行三张图从左到右依次是原图，重构图，换性别+加眼镜图，三行对应的属性强度分别为1，2，3（重构图的属性强度也相应变化）。
 
 
需要注意的是，label强度不能太高，否则图像可能会花掉，参见上图最后一行。


此外，还尝试过不同的重构误差，不同的kernel大小，不同的网络深度，是否采用autoencoder预训练。发现kernel大小和是否采用autoencoder预训练对结果影响不大，网络太浅方法会完全失效，大概是因为太浅的网络没有办法把属性和图像的编码进行剥离（语义层面的信息太浅）。重构误差换成L1收敛会更快，重构效果要好一些。重构误差换成交叉熵，图像会重构得更好，但是属性修改效果会更差一些。
下图是FaderNets的重构误差用交叉熵的结果（属性：眼镜），五行的label强度分别是1~5。
  

	FaderGAN
FaderNets难以得到清晰的结果，是因为采用了重构误差。如果在图像层面上做对抗，图像会清晰很多。因此我们提出FaderGAN模型。FaderGAN不再关注属性是否被剥离出来，而是要求decoder表达的是我们指定的属性（通过condition引入），而不是原图上的属性。判别器有两个任务，一个是分类属性，一个是判断真假。FaderGAN虽然是从FaderNets演化而来，但是它实际上就是ACGAN拿来做图像翻译。具体参见模型部分的介绍。

FaderGAN模型的实验结果：
下图分别是第一行：原图，重构图；第二行：属性取反图（属性有两个：性别+眼镜，可以看到眼镜属性没有效果）
  
 
FaderGAN跟FaderNets结合，就得到了既在特征层面，也在图像层面对抗的方法，它的目标比FaderGAN多了一点，那就是它除了要求decoder表达的是我们指定的属性之外，还要求图像编码跟属性无关。我们发现除了收敛快一点，最后的实验结果没有很显著的变化（下图分别是重构和属性取反图）：
  

此外，我们观察到，当训练到一定阶段，模型开始崩溃，开始生成鬼脸，原因在于生成器骗过了判别器，生成器学到了一种能骗过判别器的模式，判别器无能为力。
判别器之所以一直无法摆脱生成器的欺骗，原因在于它同时负责两个任务：分类属性+判断图像真假。这两个任务要求的特征是不一样的，甚至是重构到了一定程度以后，是互相排斥的。因此两个任务不要共享参数，或者不要共享太多参数，应该能够解决这个问题。这个原因也是才想明白的，还没有做实验。
   

此外，我们还注意到，各个属性是不均衡的。特别是眼镜之类的属性，戴眼镜的比不戴的要少很多倍，而且戴眼镜的人中，男性比女性要多。如果不管属性均衡与否，直接做可能导致某些case失败。为此，我们制作了一个性别+眼镜的均衡子集：(男，戴眼镜)，(男，不戴眼镜)，(女，戴眼镜)，(女，不戴眼镜)这四种组合的图像数量是相同的，也就是完全均衡的情况，这个数据集有10000+的图片。但是我们在多组不同参数配置，不同模型的实验中均没有观察到更好的结果。

	PGGAN
	模型
PGGAN采用progressive growing的方式，从小分辨率开始训练，然后逐渐增大分辨率（fade in到更高的分辨率），下图展示的是PGGAN生成器从4x4分辨率fade in到8x8分辨率的过程。
 
对应的判别器也从4x4逐步过渡到8x8：
 
1024x1024分辨率完整的网络结构如下：
 
文章：
Progressive Growing of GANs for Improved Quality, Stability, and Variation
复现代码：
91服务器：/data/rui.wu/gapeng/PG-GAN/（原版，upsample+conv）
/data/rui.wu/gapeng/RGAN/（换网络结构，采用反卷积层，去掉一些tricks）
	任务
PyTorch复现PGGAN，官方代码是基于theano(lasagna)。

	探索
基于官方代码改写的pytorch版本并没有论文的效果那么好，在分辨率达到256的时候，总是容易崩掉。下面是目前所能做到的最好实验效果（CelebA数据集，而不是CelebA-HQ数据集）：
下面的结果图，左边两列是生成图，右边两列是真实图。
128x128：
 
 

256x256：
 
 

此外，我还尝试了反卷积的网络结构，实验也很容易崩掉，实验只能稳定跑到32x32分辨率，更高分辨率会崩掉。下图是32x32分辨率的结果（左边4列是生成图，右边4列是真实图）。
 

PGGAN结合BEGAN目前没有调出效果。代码有bug，一直找不到。

	Pix2pixHD
	模型
基本框架还是pix2pix：
 
不同之处在于：
(1). 加入了coarse2fine的生成器，先训练一个global的生成器，用于生成512x1024的图像，待生成差不多了，再引入local分支，生成1024x2048的图像，原来512x1024的隐层特征和1024x2048的隐层特征做融合。这里分为几个阶段：训512p的网络，固定512p网络去训1024p网络，finetune 512p和1024p的网络。
(2). 加入了multi-scale的判别器，作者采用3个scale的判别器，能够有效帮助判别器从不同分辨率上分辨图像，而判别器更强则能指导生成器往更好的方向走。不同层次的判别器可以是结构一样的网络，只在输入的分辨率上有区别。
(3). 加入了feature matching。注意到pix2pix模型是需要配对数据的，因此可以要求生成图和groundtruth经过判别器的特征都匹配。
(4). 加入了boundary map（通过instance label计算每个物体的边界），使得生成结果能够区分同类不同instance的物体。加入了instance-level的编码，能够给生成器提供更多的目标图特征，而且是实例级别的特征。加入了VGG loss。
可以借鉴的地方：coarse2fine生成器和multi-scale判别器是通用可借鉴的设计；另外两点是针对特定任务和数据设计的，不具有通用性。

文章：
High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs
代码：
官方代码：https://github.com/NVIDIA/pix2pixHD
复现代码：91服务器/data/rui.wu/gapeng/RGAN_pix2pixhd/（仅复现了不使用instance label，不使用Encoder做instance level的编码不加VGG loss，256x512的效果）
官方代码在48服务器上：/data/xiaobing.wang/gapeng/pix2pixHD-master/
	任务
理解并复现pix2pixHD的结果。Pix2pixHD的方法可以借鉴，用到其他任务中，使得训练更稳定。
	探索
实现了pix2pixHD的coarse2fine生成器和multi-scale判别器，以及加上了真假样本在判别器上的feature matching。下图是256x512实验迭代10万步(约33个epoch)的结果，三张图依次是分割图、真实图、生成图。实验参见目录91服务器上的目录/data/rui.wu/gapeng/RGAN_pix2pixhd
 

可以看到，生成的多数内容是正常的，除了车。继续训练效果应该会更好。

存在的问题：复现的版本不包含Encoder，也不使用instance label和boundary map，因此车辆的生成效果普遍比较差，而且不能区分不同的车辆。这点在论文中也提到了。


此外，跑官方代码的效果（加上boundary map和instance-level的encoder、VGG loss）很好，训练分为三个阶段，512p网络训练、固定512p网络，训1024p网络、两个网络联合finetune，三个阶段的结果如下（每三张图为一组，按照顺序依次为分割图、真实图、生成图）：
	512p网络生成结果（更多结果参看48服务器目录/data/xiaobing.wang/gapeng/pix2pixHD-master/checkpoints/label2city_512p_feat/，这个阶段总共要训练200个epoch，100个epoch以后调整学习率。后续阶段的实验并没有在512p网络完全训练完了再开始，而是用的epoch 39的结果初始化后续训练的生成器）：
239400 step，epoch 81:
 
 
 

 
 
 

443600 step，epoch 150：
 
 
 


	固定512p网络，训1024p网络结果（更多结果参看48服务器目录/data/xiaobing.wang/gapeng/pix2pixHD-master/checkpoints/label2city_1024p_feat/web/images的前十个epoch，该阶段总共训练10个epoch）：
28100 step，epoch 10：
 
 
 


	512p网络和1024p网络联合finetune的实验结果（这个阶段总共要训练100个epoch）：
epoch 11（即finetune的第1个epoch）的结果：
 
 
 

epoch 12， step 35000（finetune的第2个epoch）：
 
 
 

epoch 83，step 245400（finetune的第73个epoch）：
 
 
 


